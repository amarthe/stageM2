
@article{achab_robustness_2021,
  title    = {Robustness and risk management via distributional dynamic programming},
  url      = {http://arxiv.org/abs/2112.15430},
  abstract = {In dynamic programming (DP) and reinforcement learning (RL), an agent learns to act optimally in terms of expected long-term return by sequentially interacting with its environment modeled by a Markov decision process (MDP). More generally in distributional reinforcement learning (DRL), the focus is on the whole distribution of the return, not just its expectation. Although DRL-based methods produced state-of-the-art performance in RL with function approximation, they involve additional quantities (compared to the non-distributional setting) that are still not well understood. As a first contribution, we introduce a new class of distributional operators, together with a practical DP algorithm for policy evaluation, that come with a robust MDP interpretation. Indeed, our approach reformulates through an augmented state space where each state is split into a worst-case substate and a best-case substate, whose values are maximized by safe and risky policies respectively. Finally, we derive distributional operators and DP algorithms solving a new control task: How to distinguish safe from risky optimal actions in order to break ties in the space of optimal policies?},
  language = {en},
  urldate  = {2022-04-07},
  journal  = {arXiv:2112.15430 [cs, math]},
  author   = {Achab, Mastane and Neu, Gergely},
  month    = dec,
  year     = {2021},
  note     = {arXiv: 2112.15430},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control},
  file     = {Achab et Neu - 2021 - Robustness and risk management via distributional .pdf:C\:\\Users\\Alex\\Zotero\\storage\\RX6ZIP7U\\Achab et Neu - 2021 - Robustness and risk management via distributional .pdf:application/pdf}
}

@article{bellemare_distributional_2017,
  title    = {A {Distributional} {Perspectiv e} on {Reinforcement} {Learning}},
  url      = {http://arxiv.org/abs/1707.06887},
  abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a speciﬁc purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a signiﬁcant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman’s equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
  language = {en},
  urldate  = {2022-04-07},
  journal  = {arXiv:1707.06887 [cs, stat]},
  author   = {Bellemare, Marc G. and Dabney, Will and Munos, Rémi},
  month    = jul,
  year     = {2017},
  note     = {arXiv: 1707.06887},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
  annote   = {Comment: ICML 2017},
  file     = {Bellemare et al. - 2017 - A Distributional Perspective on Reinforcement Lear.pdf:C\:\\Users\\Alex\\Zotero\\storage\\NF845S3C\\Bellemare et al. - 2017 - A Distributional Perspective on Reinforcement Lear.pdf:application/pdf}
}

@article{dabney_distributional_2017,
  title    = {Distributional {Reinforcement} {Learning} with {Quantile} {Regression}},
  url      = {http://arxiv.org/abs/1710.10044},
  abstract = {In reinforcement learning an agent interacts with the environment by taking actions and observing the next state and reward. When sampled probabilistically, these state transitions, rewards, and actions can all induce randomness in the observed long-term return. Traditionally, reinforcement learning algorithms average over this randomness to estimate the value function. In this paper, we build on recent work advocating a distributional approach to reinforcement learning in which the distribution over returns is modeled explicitly instead of only estimating the mean. That is, we examine methods of learning the value distribution instead of the value function. We give results that close a number of gaps between the theoretical and algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we extend existing results to the approximate distribution setting. Second, we present a novel distributional reinforcement learning algorithm consistent with our theoretical formulation. Finally, we evaluate this new algorithm on the Atari 2600 games, observing that it significantly outperforms many of the recent improvements on DQN, including the related distributional algorithm C51.},
  urldate  = {2022-04-12},
  journal  = {arXiv:1710.10044 [cs, stat]},
  author   = {Dabney, Will and Rowland, Mark and Bellemare, Marc G. and Munos, Rémi},
  month    = oct,
  year     = {2017},
  note     = {arXiv: 1710.10044},
  keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
  file     = {arXiv Fulltext PDF:C\:\\Users\\Alex\\Zotero\\storage\\23VMF8AN\\Dabney et al. - 2017 - Distributional Reinforcement Learning with Quantil.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Alex\\Zotero\\storage\\97KJD2YI\\1710.html:text/html}
}

@InProceedings{pmlr-v49-garivier16a,
  title = 	 {Optimal Best Arm Identification with Fixed Confidence},
  author = 	 {Garivier, Aurélien and Kaufmann, Emilie},
  booktitle = 	 {29th Annual Conference on Learning Theory},
  pages = 	 {998--1027},
  year = 	 {2016},
  editor = 	 {Feldman, Vitaly and Rakhlin, Alexander and Shamir, Ohad},
  volume = 	 {49},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Columbia University, New York, New York, USA},
  month = 	 {23--26 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v49/garivier16a.pdf},
  url = 	 {https://proceedings.mlr.press/v49/garivier16a.html},
  abstract = 	 {We give a complete characterization of the complexity of best-arm identification in one-parameter bandit problems. We prove a new, tight lower bound on the sample complexity. We propose the ‘Track-and-Stop’ strategy, which we prove to be asymptotically optimal. It consists in a new sampling rule (which tracks the optimal proportions of arm draws highlighted by the lower bound) and in a stopping rule named after Chernoff, for which we give a new analysis.}
}


@article{lyle_comparative_2019,
  title     = {A {Comparative} {Analysis} of {Expected} and {Distributional} {Reinforcement} {Learning}},
  volume    = {33},
  copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
  issn      = {2374-3468},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/4365},
  doi       = {10.1609/aaai.v33i01.33014504},
  abstract  = {Since their introduction a year ago, distributional approaches to reinforcement learning (distributional RL) have produced strong results relative to the standard approach which models expected values (expected RL). However, aside from convergence guarantees, there have been few theoretical results investigating the reasons behind the improvements distributional RL provides. In this paper we begin the investigation into this fundamental question by analyzing the differences in the tabular, linear approximation, and non-linear approximation settings. We prove that in many realizations of the tabular and linear approximation settings, distributional RL behaves exactly the same as expected RL. In cases where the two methods behave differently, distributional RL can in fact hurt performance when it does not induce identical behaviour. We then continue with an empirical analysis comparing distributional and expected RL methods in control settings with non-linear approximators to tease apart where the improvements from distributional RL methods are coming from.},
  language  = {en},
  number    = {01},
  urldate   = {2022-05-12},
  journal   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author    = {Lyle, Clare and Bellemare, Marc G. and Castro, Pablo Samuel},
  month     = jul,
  year      = {2019},
  note      = {Number: 01},
  pages     = {4504--4511},
  file      = {Full Text PDF:C\:\\Users\\Alex\\Zotero\\storage\\IGPBTX3L\\Lyle et al. - 2019 - A Comparative Analysis of Expected and Distributio.pdf:application/pdf}
}

@article{morimura_parametric_2012,
  title    = {Parametric {Return} {Density} {Estimation} for {Reinforcement} {Learning}},
  url      = {http://arxiv.org/abs/1203.3497},
  abstract = {Most conventional Reinforcement Learning (RL) algorithms aim to optimize decision-making rules in terms of the expected returns. However, especially for risk management purposes, other risk-sensitive criteria such as the value-at-risk or the expected shortfall are sometimes preferred in real applications. Here, we describe a parametric method for estimating density of the returns, which allows us to handle various criteria in a unified manner. We first extend the Bellman equation for the conditional expected return to cover a conditional probability density of the returns. Then we derive an extension of the TD-learning algorithm for estimating the return densities in an unknown environment. As test instances, several parametric density estimation algorithms are presented for the Gaussian, Laplace, and skewed Laplace distributions. We show that these algorithms lead to risk-sensitive as well as robust RL paradigms through numerical experiments.},
  urldate  = {2022-04-19},
  journal  = {arXiv:1203.3497 [cs, stat]},
  author   = {Morimura, Tetsuro and Sugiyama, Masashi and Kashima, Hisashi and Hachiya, Hirotaka and Tanaka, Toshiyuki},
  month    = mar,
  year     = {2012},
  note     = {arXiv: 1203.3497},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  annote   = {Comment: Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence (UAI2010)},
  file     = {arXiv Fulltext PDF:C\:\\Users\\Alex\\Zotero\\storage\\U9S5I9HJ\\Morimura et al. - 2012 - Parametric Return Density Estimation for Reinforce.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Alex\\Zotero\\storage\\ZJNFIPF6\\1203.html:text/html}
}

@inproceedings{rowland_analysis_2018,
  title     = {An {Analysis} of {Categorical} {Distributional} {Reinforcement} {Learning}},
  url       = {https://proceedings.mlr.press/v84/rowland18a.html},
  abstract  = {Distributional approaches to value-based reinforcement learning model the entire distribution of returns, rather than just their expected values, and have recently been shown to yield state-of-the-art empirical performance. This was demonstrated by the recently proposed C51 algorithm, based on categorical distributional reinforcement learning (CDRL) [Bellemare et al., 2017]. However, the theoretical properties of CDRL algorithms are not yet well understood. In this paper, we introduce a framework to analyse CDRL algorithms, establish the importance of the projected distributional Bellman operator in distributional RL, draw fundamental connections between CDRL and the Cramer distance, and give a proof of convergence for sample-based categorical distributional reinforcement learning algorithms.},
  language  = {en},
  urldate   = {2022-04-27},
  booktitle = {Proceedings of the {Twenty}-{First} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
  publisher = {PMLR},
  author    = {Rowland, Mark and Bellemare, Marc and Dabney, Will and Munos, Remi and Teh, Yee Whye},
  month     = mar,
  year      = {2018},
  note      = {ISSN: 2640-3498},
  pages     = {29--37},
  file      = {Full Text PDF:C\:\\Users\\Alex\\Zotero\\storage\\5AEJP9BA\\Rowland et al. - 2018 - An Analysis of Categorical Distributional Reinforc.pdf:application/pdf;Supplementary PDF:C\:\\Users\\Alex\\Zotero\\storage\\HHLJSRIP\\Rowland et al. - 2018 - An Analysis of Categorical Distributional Reinforc.pdf:application/pdf}
}